{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be399ad",
   "metadata": {},
   "source": [
    "• Read all the problem statements, notes carefully and scrape the required data using any web scraping tool of\n",
    "your choice.\n",
    "• You have to handle commonly occurring EXCEPTIONS by using exception handling programing. To get\n",
    "information about selenium Exceptions. You may visit following links:\n",
    "1. https://selenium-python.readthedocs.io/api.html\n",
    "2. https://www.guru99.com/exception-handling-selenium.html\n",
    "3. https://stackoverflow.com/questions/38022658/selenium-python-handling-no-such-elementexception/38023345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc57146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a2efc",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0c88c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cee49cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list for scraping the data\n",
    "\n",
    "Rank = []\n",
    "Name = []\n",
    "Artist = []\n",
    "Date = []\n",
    "Views = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "277b6841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views (in Billions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Baby Shark Dance</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>12.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Despacito</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Johny Johny Yes Papa</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>Bath Song</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>See You Again</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>Phonics Song with Two Words</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>Wheels on the Bus</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>5.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>Uptown Funk</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>Learning Colors – Colorful Eggs on a Farm</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>Gangnam Style</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>Masha and the Bear – Recipe for Disaster</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>Dame Tu Cosita</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>Axel F</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>3.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>Roar</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>Counting Stars</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>Sorry</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>Baa Baa Black Sheep</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>Thinking Out Loud</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>Waka Waka (This Time for Africa)</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>Dark Horse</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>Lakdi Ki Kathi</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>June 14, 2018</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>Faded</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>Let Her Go</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>Girls Like You</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>Humpty the train on a fruits ride</td>\n",
       "      <td>Kiddiestv Hindi – Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>Lean On</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>Bailando</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                       Name  \\\n",
       "0    1.                           Baby Shark Dance   \n",
       "1    2.                                  Despacito   \n",
       "2    3.                       Johny Johny Yes Papa   \n",
       "3    4.                                  Bath Song   \n",
       "4    5.                               Shape of You   \n",
       "5    6.                              See You Again   \n",
       "6    7.                Phonics Song with Two Words   \n",
       "7    8.                          Wheels on the Bus   \n",
       "8    9.                                Uptown Funk   \n",
       "9   10.  Learning Colors – Colorful Eggs on a Farm   \n",
       "10  11.                              Gangnam Style   \n",
       "11  12.   Masha and the Bear – Recipe for Disaster   \n",
       "12  13.                             Dame Tu Cosita   \n",
       "13  14.                                     Axel F   \n",
       "14  15.                                      Sugar   \n",
       "15  16.                                       Roar   \n",
       "16  17.                             Counting Stars   \n",
       "17  18.                                      Sorry   \n",
       "18  19.                        Baa Baa Black Sheep   \n",
       "19  20.                          Thinking Out Loud   \n",
       "20  21.           Waka Waka (This Time for Africa)   \n",
       "21  22.                                 Dark Horse   \n",
       "22  23.                             Lakdi Ki Kathi   \n",
       "23  24.                                      Faded   \n",
       "24  25.                                    Perfect   \n",
       "25  26.                                 Let Her Go   \n",
       "26  27.                             Girls Like You   \n",
       "27  28.          Humpty the train on a fruits ride   \n",
       "28  29.                                    Lean On   \n",
       "29  30.                                   Bailando   \n",
       "\n",
       "                                           Artist        Upload Date  \\\n",
       "0     Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016   \n",
       "1                                      Luis Fonsi   January 12, 2017   \n",
       "2                                     LooLoo Kids    October 8, 2016   \n",
       "3                      Cocomelon – Nursery Rhymes        May 2, 2018   \n",
       "4                                      Ed Sheeran   January 30, 2017   \n",
       "5                                     Wiz Khalifa      April 6, 2015   \n",
       "6                                       ChuChu TV      March 6, 2014   \n",
       "7                      Cocomelon – Nursery Rhymes       May 24, 2018   \n",
       "8                                     Mark Ronson  November 19, 2014   \n",
       "9                                     Miroshka TV  February 27, 2018   \n",
       "10                                            Psy      July 15, 2012   \n",
       "11                                     Get Movies   January 31, 2012   \n",
       "12                                      El Chombo      April 5, 2018   \n",
       "13                                     Crazy Frog      June 16, 2009   \n",
       "14                                       Maroon 5   January 14, 2015   \n",
       "15                                     Katy Perry  September 5, 2013   \n",
       "16                                    OneRepublic       May 31, 2013   \n",
       "17                                  Justin Bieber   October 22, 2015   \n",
       "18                     Cocomelon – Nursery Rhymes      June 25, 2018   \n",
       "19                                     Ed Sheeran    October 7, 2014   \n",
       "20                                        Shakira       June 4, 2010   \n",
       "21                                     Katy Perry  February 20, 2014   \n",
       "22                                   Jingle Toons      June 14, 2018   \n",
       "23                                    Alan Walker   December 3, 2015   \n",
       "24                                     Ed Sheeran   November 9, 2017   \n",
       "25                                      Passenger      July 25, 2012   \n",
       "26                                       Maroon 5       May 31, 2018   \n",
       "27  Kiddiestv Hindi – Nursery Rhymes & Kids Songs   January 26, 2018   \n",
       "28                                    Major Lazer     March 22, 2015   \n",
       "29                               Enrique Iglesias     April 11, 2014   \n",
       "\n",
       "   Views (in Billions)  \n",
       "0                12.85  \n",
       "1                 8.16  \n",
       "2                 6.70  \n",
       "3                 6.20  \n",
       "4                 6.00  \n",
       "5                 5.89  \n",
       "6                 5.30  \n",
       "7                 5.24  \n",
       "8                 4.92  \n",
       "9                 4.89  \n",
       "10                4.80  \n",
       "11                4.55  \n",
       "12                4.35  \n",
       "13                3.91  \n",
       "14                3.87  \n",
       "15                3.80  \n",
       "16                3.79  \n",
       "17                3.66  \n",
       "18                3.64  \n",
       "19                3.60  \n",
       "20                3.59  \n",
       "21                3.52  \n",
       "22                3.48  \n",
       "23                3.45  \n",
       "24                3.45  \n",
       "25                3.44  \n",
       "26                3.42  \n",
       "27                3.41  \n",
       "28                3.38  \n",
       "29                3.38  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scraping Rank of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[1]\"):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"-\")\n",
    "        \n",
    "# Scraping Name of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[2]\"):\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append(\"-\")\n",
    "        \n",
    "# Scraping Artist of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[3]\"):\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Artist.append(\"-\")\n",
    "        \n",
    "# Scraping Upload_Date of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[5]\"):\n",
    "        Date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Date.append(\"-\")\n",
    "        \n",
    "# Scraping Views of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[4]\"):\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Views.append(\"-\")\n",
    "        \n",
    "# creating DataFrame for scraped data\n",
    "Wiki = pd.DataFrame({})\n",
    "Wiki['Rank'] = Rank\n",
    "Wiki['Name'] = Name\n",
    "Wiki['Artist'] = Artist\n",
    "Wiki['Upload Date'] = Date\n",
    "Wiki['Views (in Billions)'] = Views\n",
    "\n",
    "# removing stray numbers from Name column\n",
    "Wiki.Name = Wiki.Name.apply(lambda x:x[:-4].strip('\"'))\n",
    "Wiki\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0a93378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30 30 30\n"
     ]
    }
   ],
   "source": [
    "print(len(Rank),\n",
    "len(Name),\n",
    "len(Artist),\n",
    "len(Date),\n",
    "len(Views))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3835e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491ac60",
   "metadata": {},
   "source": [
    "2. Scrape the details teamIndia’sinternationalfixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1stODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "063ddf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url=('https://www.bcci.tv/')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59fcae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn=driver.find_element(By.XPATH,\"//a[@class='nav-link active']\").click()\n",
    "\n",
    "\n",
    "\n",
    "# creating empty lists for scraping the data\n",
    "Match_Title = []\n",
    "Series = []\n",
    "Place = []\n",
    "Date = []\n",
    "Time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f27de797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st Test -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>1st Test - Windsor Park, Dominica</td>\n",
       "      <td>12 JUL 2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd Test -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>2nd Test - Queen's Park Oval, Trinidad</td>\n",
       "      <td>20 JUL 2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1st ODI -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>1st ODI - Kensington Oval, Barbados</td>\n",
       "      <td>27 JUL 2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2nd ODI -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>2nd ODI - Kensington Oval, Barbados</td>\n",
       "      <td>29 JUL 2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3rd ODI -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>3rd ODI - Brian Lara Stadium, Trinidad</td>\n",
       "      <td>1 AUG 2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1st T20I -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>1st T20I - Brian Lara Stadium, Trinidad</td>\n",
       "      <td>3 AUG 2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2nd T20I -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>2nd T20I - National Stadium, Guyana</td>\n",
       "      <td>6 AUG 2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3rd T20I -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>3rd T20I - National Stadium, Guyana</td>\n",
       "      <td>8 AUG 2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Match Title                          Series  \\\n",
       "0  1st Test -  INDIA TOUR OF WEST INDIES 2023   \n",
       "1  2nd Test -  INDIA TOUR OF WEST INDIES 2023   \n",
       "2   1st ODI -  INDIA TOUR OF WEST INDIES 2023   \n",
       "3   2nd ODI -  INDIA TOUR OF WEST INDIES 2023   \n",
       "4   3rd ODI -  INDIA TOUR OF WEST INDIES 2023   \n",
       "5  1st T20I -  INDIA TOUR OF WEST INDIES 2023   \n",
       "6  2nd T20I -  INDIA TOUR OF WEST INDIES 2023   \n",
       "7  3rd T20I -  INDIA TOUR OF WEST INDIES 2023   \n",
       "\n",
       "                                     Place         Date  Time  \n",
       "0        1st Test - Windsor Park, Dominica  12 JUL 2023  2023  \n",
       "1   2nd Test - Queen's Park Oval, Trinidad  20 JUL 2023  2023  \n",
       "2      1st ODI - Kensington Oval, Barbados  27 JUL 2023  2023  \n",
       "3      2nd ODI - Kensington Oval, Barbados  29 JUL 2023  2023  \n",
       "4   3rd ODI - Brian Lara Stadium, Trinidad   1 AUG 2023  2023  \n",
       "5  1st T20I - Brian Lara Stadium, Trinidad   3 AUG 2023  2023  \n",
       "6      2nd T20I - National Stadium, Guyana   6 AUG 2023  2023  \n",
       "7      3rd T20I - National Stadium, Guyana   8 AUG 2023  2023  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in driver.find_elements(By.XPATH,\"//span[@class='matchOrderText ng-binding ng-scope']\"):\n",
    "    Match_Title.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements(By.XPATH,\"//div[@class='match-card-top']/h5\"):\n",
    "    Series.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements(By.XPATH,\"//div[@class='match-place ng-scope']\"):\n",
    "    Place.append(i.text)\n",
    "        \n",
    "for i in driver.find_elements(By.XPATH,\"//div[@class='match-dates ng-binding']\"):\n",
    "    Date.append(i.text.replace('\\n',' '))\n",
    "\n",
    "date=[i.split(' ',3)[:3] for i in Date]\n",
    "date=[' '.join(i) for i in date]\n",
    "Time=[i.split(' ',3)[-1] for i in Date]\n",
    "\n",
    "# creating data frame\n",
    "fixture=pd.DataFrame({'Match Title': Match_Title,\n",
    "                          \"Series\": Series,\n",
    "                          \"Place\": Place,\n",
    "                          \"Date\": date,\n",
    "                          \"Time\": Time})\n",
    "fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7059ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b1e07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12880f6",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP ofIndia fromstatisticstime.com. \n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7488062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://statisticstimes.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d70d714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on Economy button\n",
    "driver.find_element(By.XPATH,\"//div[@class='navbar']/div[2]/button\").click()\n",
    "\n",
    "# clicking on India\n",
    "driver.find_element(By.XPATH,\"//div[@class='dropdown-content']/a[3]\").click()\n",
    "time.sleep(3)\n",
    "\n",
    "# clicking on GDP of Indian Economy\n",
    "GDP = driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8203fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP at current price (19-20)</th>\n",
       "      <th>GSDP at current price (18-19)</th>\n",
       "      <th>Share (18-19)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>29</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>28,391</td>\n",
       "      <td>25,141</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>17,060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>24,534</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>22,488</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>24,424</td>\n",
       "      <td>20,947</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>17,797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP at current price (19-20)  \\\n",
       "0     1                Maharashtra                             -   \n",
       "1     2                 Tamil Nadu                     1,845,853   \n",
       "2     3              Uttar Pradesh                     1,687,818   \n",
       "3     4                    Gujarat                             -   \n",
       "4     5                  Karnataka                     1,631,977   \n",
       "..  ...                        ...                           ...   \n",
       "61   29                     Sikkim                        28,391   \n",
       "62   30                   Nagaland                             -   \n",
       "63   31          Arunachal Pradesh                             -   \n",
       "64   32                    Mizoram                        24,424   \n",
       "65   33  Andaman & Nicobar Islands                             -   \n",
       "\n",
       "   GSDP at current price (18-19) Share (18-19) GDP($ billion)  \n",
       "0                      2,632,792        13.94%        399.921  \n",
       "1                      1,630,208         8.63%        247.629  \n",
       "2                      1,584,764         8.39%        240.726  \n",
       "3                      1,502,899         7.96%        228.290  \n",
       "4                      1,493,127         7.91%        226.806  \n",
       "..                           ...           ...            ...  \n",
       "61                        25,141         0.15%         17,060  \n",
       "62                        24,534         0.15%              -  \n",
       "63                        22,488         0.13%              -  \n",
       "64                        20,947         0.13%         17,797  \n",
       "65                             -             -              -  \n",
       "\n",
       "[66 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty list\n",
    "Rank = []\n",
    "State = []\n",
    "GSDP1 = []\n",
    "GSDP2 = []\n",
    "Share = []\n",
    "GDP_billion = []\n",
    "\n",
    "# scraping Rank\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[1]\"):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"_\")\n",
    "    \n",
    "# scraping State\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[2]\"):\n",
    "        State.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    State.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (19-20)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[3]\"):\n",
    "        GSDP1.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP1.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[4]\"):\n",
    "        GSDP2.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP2.append(\"_\")\n",
    "    \n",
    "# scraping Share (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[5]\"):\n",
    "        Share.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Share.append(\"_\")\n",
    "    \n",
    "# scraping GDP $ billion\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[6]\"):\n",
    "        GDP_billion.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDP_billion.append(\"_\")\n",
    "    \n",
    "    \n",
    "# creating DataFrame from the scraped data\n",
    "GDP = pd.DataFrame({})\n",
    "GDP['Rank'] = Rank\n",
    "GDP['State'] = State\n",
    "GDP['GSDP at current price (19-20)'] = GSDP1\n",
    "GDP['GSDP at current price (18-19)'] = GSDP2\n",
    "GDP['Share (18-19)'] = Share\n",
    "GDP['GDP($ billion)'] = GDP_billion\n",
    "GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "222f87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d378317d",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "ASSIGNMENT\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "abea45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://github.com/\")\n",
    "driver.get(url)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "63645802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on the signin option\n",
    "signin=driver.find_element(By.XPATH,\"//div[@class='position-relative mr-lg-3 d-lg-inline-block']\").click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a507772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserting email\n",
    "email=driver.find_element(By.XPATH,\"//input[@class='form-control input-block js-login-field']\")\n",
    "email.send_keys(\"rimyrim@gmail.com\")\n",
    "#inserting password\n",
    "password=driver.find_element(By.XPATH,\"//input[@class='form-control form-control input-block js-password-field']\")\n",
    "password.send_keys(\"GoodNewsAnu!!22qq\")\n",
    "#clicking on signin button\n",
    "signin_btn=driver.find_element(By.XPATH,\"//input[@class='btn btn-primary btn-block js-sign-in-button']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ab182795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on menu button\n",
    "mnu_btn=driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[1]/deferred-side-panel/include-fragment/button\").click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a0fea71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting explore button and clicking on it\n",
    "explore = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[1]/deferred-side-panel/div/modal-dialog/div[3]/nav/nav-list/ul/li[1]/a/span[2]\").click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0604ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting trending option\n",
    "trend_url = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/main/div[2]/div/div/div[3]/div[1]/div[1]/h3/a\")\n",
    "urls = trend_url.get_attribute(\"href\")\n",
    "driver.get(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "973e63b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository URL</th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Contributors Count</th>\n",
       "      <th>Language Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/ChaoningZhang/MobileSAM</td>\n",
       "      <td>ChaoningZhang / MobileSAM</td>\n",
       "      <td>This is the offiicial code for Faster Segment ...</td>\n",
       "      <td>ChaoningZhang Chaoning Zhang\\nqiaoyu1002 Yu Qiao</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/ramonvc/freegpt-webui</td>\n",
       "      <td>ramonvc / freegpt-webui</td>\n",
       "      <td>GPT 3.5/4 with a Chat Web UI. No API key requi...</td>\n",
       "      <td>ramonvc Ramon Victor Cardoso\\ntiansztiansz\\nel...</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/THUDM/ChatGLM2-6B</td>\n",
       "      <td>THUDM / ChatGLM2-6B</td>\n",
       "      <td>ChatGLM2-6B: An Open Bilingual Chat LLM | 开源双语...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/PowerShell/PowerShell</td>\n",
       "      <td>PowerShell / PowerShell</td>\n",
       "      <td>PowerShell for every system!</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/XingangPan/DragGAN</td>\n",
       "      <td>XingangPan / DragGAN</td>\n",
       "      <td>Official Code for DragGAN (SIGGRAPH 2023)</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://github.com/hiyouga/ChatGLM-Efficient-T...</td>\n",
       "      <td>hiyouga / ChatGLM-Efficient-Tuning</td>\n",
       "      <td>Fine-tuning ChatGLM-6B with PEFT | 基于 PEFT 的高效...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://github.com/facebook/folly</td>\n",
       "      <td>facebook / folly</td>\n",
       "      <td>An open-source C++ library developed and used ...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://github.com/THUDM/ChatGLM-6B</td>\n",
       "      <td>THUDM / ChatGLM-6B</td>\n",
       "      <td>ChatGLM-6B: An Open Bilingual Dialogue Languag...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://github.com/practical-tutorials/project...</td>\n",
       "      <td>practical-tutorials / project-based-learning</td>\n",
       "      <td>Curated list of project-based tutorials</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://github.com/sohamkamani/javascript-desi...</td>\n",
       "      <td>sohamkamani / javascript-design-patterns-for-h...</td>\n",
       "      <td>An ultra-simplified explanation of design patt...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://github.com/papers-we-love/papers-we-love</td>\n",
       "      <td>papers-we-love / papers-we-love</td>\n",
       "      <td>Papers from the computer science community to ...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://github.com/cvg/LightGlue</td>\n",
       "      <td>cvg / LightGlue</td>\n",
       "      <td>LightGlue: Local Feature Matching at Light Speed</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://github.com/turboderp/exllama</td>\n",
       "      <td>turboderp / exllama</td>\n",
       "      <td>A more memory-efficient rewrite of the HF tran...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://github.com/fuqiuluo/unidbg-fetch-qsign</td>\n",
       "      <td>fuqiuluo / unidbg-fetch-qsign</td>\n",
       "      <td>获取QQSign通过Unidbg</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://github.com/jasontaylordev/CleanArchite...</td>\n",
       "      <td>jasontaylordev / CleanArchitecture</td>\n",
       "      <td>Clean Architecture Solution Template for ASP.N...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://github.com/toeverything/AFFiNE</td>\n",
       "      <td>toeverything / AFFiNE</td>\n",
       "      <td>There can be more than Notion and Miro. AFFiNE...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://github.com/ripienaar/free-for-dev</td>\n",
       "      <td>ripienaar / free-for-dev</td>\n",
       "      <td>A list of SaaS, PaaS and IaaS offerings that h...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://github.com/EbookFoundation/free-progra...</td>\n",
       "      <td>EbookFoundation / free-programming-books</td>\n",
       "      <td>📚 Freely available programming books</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://github.com/chat2db/Chat2DB</td>\n",
       "      <td>chat2db / Chat2DB</td>\n",
       "      <td>🔥 🔥 🔥 An intelligent and versatile general-pur...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://github.com/PlexPt/awesome-chatgpt-prom...</td>\n",
       "      <td>PlexPt / awesome-chatgpt-prompts-zh</td>\n",
       "      <td>ChatGPT 中文调教指南。各种场景使用指南。学习怎么让它听你的话。</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://github.com/labmlai/annotated_deep_lear...</td>\n",
       "      <td>labmlai / annotated_deep_learning_paper_implem...</td>\n",
       "      <td>🧑‍🏫 59 Implementations/tutorials of deep learn...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://github.com/microsoft/Web-Dev-For-Begin...</td>\n",
       "      <td>microsoft / Web-Dev-For-Beginners</td>\n",
       "      <td>24 Lessons, 12 Weeks, Get Started as a Web Dev...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://github.com/dragonflydb/dragonfly</td>\n",
       "      <td>dragonflydb / dragonfly</td>\n",
       "      <td>A modern replacement for Redis and Memcached</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://github.com/wgwang/LLMs-In-China</td>\n",
       "      <td>wgwang / LLMs-In-China</td>\n",
       "      <td>中国大模型</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://github.com/StanGirard/quivr</td>\n",
       "      <td>StanGirard / quivr</td>\n",
       "      <td>🧠 Dump all your files and thoughts into your p...</td>\n",
       "      <td>-</td>\n",
       "      <td>[Jupyter Notebook, Python, JavaScript, CSS, HT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Repository URL  \\\n",
       "0          https://github.com/ChaoningZhang/MobileSAM   \n",
       "1            https://github.com/ramonvc/freegpt-webui   \n",
       "2                https://github.com/THUDM/ChatGLM2-6B   \n",
       "3            https://github.com/PowerShell/PowerShell   \n",
       "4               https://github.com/XingangPan/DragGAN   \n",
       "5   https://github.com/hiyouga/ChatGLM-Efficient-T...   \n",
       "6                   https://github.com/facebook/folly   \n",
       "7                 https://github.com/THUDM/ChatGLM-6B   \n",
       "8   https://github.com/practical-tutorials/project...   \n",
       "9   https://github.com/sohamkamani/javascript-desi...   \n",
       "10   https://github.com/papers-we-love/papers-we-love   \n",
       "11                   https://github.com/cvg/LightGlue   \n",
       "12               https://github.com/turboderp/exllama   \n",
       "13     https://github.com/fuqiuluo/unidbg-fetch-qsign   \n",
       "14  https://github.com/jasontaylordev/CleanArchite...   \n",
       "15             https://github.com/toeverything/AFFiNE   \n",
       "16          https://github.com/ripienaar/free-for-dev   \n",
       "17  https://github.com/EbookFoundation/free-progra...   \n",
       "18                 https://github.com/chat2db/Chat2DB   \n",
       "19  https://github.com/PlexPt/awesome-chatgpt-prom...   \n",
       "20  https://github.com/labmlai/annotated_deep_lear...   \n",
       "21  https://github.com/microsoft/Web-Dev-For-Begin...   \n",
       "22           https://github.com/dragonflydb/dragonfly   \n",
       "23            https://github.com/wgwang/LLMs-In-China   \n",
       "24                https://github.com/StanGirard/quivr   \n",
       "\n",
       "                                     Repository Title  \\\n",
       "0                           ChaoningZhang / MobileSAM   \n",
       "1                             ramonvc / freegpt-webui   \n",
       "2                                 THUDM / ChatGLM2-6B   \n",
       "3                             PowerShell / PowerShell   \n",
       "4                                XingangPan / DragGAN   \n",
       "5                  hiyouga / ChatGLM-Efficient-Tuning   \n",
       "6                                    facebook / folly   \n",
       "7                                  THUDM / ChatGLM-6B   \n",
       "8        practical-tutorials / project-based-learning   \n",
       "9   sohamkamani / javascript-design-patterns-for-h...   \n",
       "10                    papers-we-love / papers-we-love   \n",
       "11                                    cvg / LightGlue   \n",
       "12                                turboderp / exllama   \n",
       "13                      fuqiuluo / unidbg-fetch-qsign   \n",
       "14                 jasontaylordev / CleanArchitecture   \n",
       "15                              toeverything / AFFiNE   \n",
       "16                           ripienaar / free-for-dev   \n",
       "17           EbookFoundation / free-programming-books   \n",
       "18                                  chat2db / Chat2DB   \n",
       "19                PlexPt / awesome-chatgpt-prompts-zh   \n",
       "20  labmlai / annotated_deep_learning_paper_implem...   \n",
       "21                  microsoft / Web-Dev-For-Beginners   \n",
       "22                            dragonflydb / dragonfly   \n",
       "23                             wgwang / LLMs-In-China   \n",
       "24                                 StanGirard / quivr   \n",
       "\n",
       "                               Repository Description  \\\n",
       "0   This is the offiicial code for Faster Segment ...   \n",
       "1   GPT 3.5/4 with a Chat Web UI. No API key requi...   \n",
       "2   ChatGLM2-6B: An Open Bilingual Chat LLM | 开源双语...   \n",
       "3                        PowerShell for every system!   \n",
       "4           Official Code for DragGAN (SIGGRAPH 2023)   \n",
       "5   Fine-tuning ChatGLM-6B with PEFT | 基于 PEFT 的高效...   \n",
       "6   An open-source C++ library developed and used ...   \n",
       "7   ChatGLM-6B: An Open Bilingual Dialogue Languag...   \n",
       "8             Curated list of project-based tutorials   \n",
       "9   An ultra-simplified explanation of design patt...   \n",
       "10  Papers from the computer science community to ...   \n",
       "11   LightGlue: Local Feature Matching at Light Speed   \n",
       "12  A more memory-efficient rewrite of the HF tran...   \n",
       "13                                   获取QQSign通过Unidbg   \n",
       "14  Clean Architecture Solution Template for ASP.N...   \n",
       "15  There can be more than Notion and Miro. AFFiNE...   \n",
       "16  A list of SaaS, PaaS and IaaS offerings that h...   \n",
       "17               📚 Freely available programming books   \n",
       "18  🔥 🔥 🔥 An intelligent and versatile general-pur...   \n",
       "19                ChatGPT 中文调教指南。各种场景使用指南。学习怎么让它听你的话。   \n",
       "20  🧑‍🏫 59 Implementations/tutorials of deep learn...   \n",
       "21  24 Lessons, 12 Weeks, Get Started as a Web Dev...   \n",
       "22       A modern replacement for Redis and Memcached   \n",
       "23                                              中国大模型   \n",
       "24  🧠 Dump all your files and thoughts into your p...   \n",
       "\n",
       "                                   Contributors Count  \\\n",
       "0    ChaoningZhang Chaoning Zhang\\nqiaoyu1002 Yu Qiao   \n",
       "1   ramonvc Ramon Victor Cardoso\\ntiansztiansz\\nel...   \n",
       "2                                                   -   \n",
       "3                                                   -   \n",
       "4                                                   -   \n",
       "5                                                   -   \n",
       "6                                                   -   \n",
       "7                                                   -   \n",
       "8                                                   -   \n",
       "9                                                   -   \n",
       "10                                                  -   \n",
       "11                                                  -   \n",
       "12                                                  -   \n",
       "13                                                  -   \n",
       "14                                                  -   \n",
       "15                                                  -   \n",
       "16                                                  -   \n",
       "17                                                  -   \n",
       "18                                                  -   \n",
       "19                                                  -   \n",
       "20                                                  -   \n",
       "21                                                  -   \n",
       "22                                                  -   \n",
       "23                                                  -   \n",
       "24                                                  -   \n",
       "\n",
       "                                        Language Used  \n",
       "0   [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "1   [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "2   [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "3   [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "4   [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "5   [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "6   [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "7   [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "8   [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "9   [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "10  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "11  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "12  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "13  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "14  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "15  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "16  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "17  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "18  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "19  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "20  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "21  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "22  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "23  [Jupyter Notebook, Python, JavaScript, CSS, HT...  \n",
       "24  [Jupyter Notebook, Python, JavaScript, CSS, HT...  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty list\n",
    "URLs = []\n",
    "repository_title = []\n",
    "Description = []\n",
    "Contributors = []\n",
    "Language = []\n",
    "lang = []\n",
    "\n",
    "\n",
    "# fetching urls for each repository\n",
    "repository = driver.find_elements(By.XPATH,\"//h2[@class='h3 lh-condensed']/a\")\n",
    "for i in repository:\n",
    "    URLs.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "# scraping Repository title data\n",
    "title = driver.find_elements(By.XPATH,\"//h2[@class='h3 lh-condensed']\")\n",
    "for i in title:\n",
    "    repository_title.append(i.text)\n",
    "    \n",
    "# scraping data from all repository page\n",
    "for i in URLs:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)     \n",
    "# scraping Repository Description data \n",
    "    try:\n",
    "        desc = driver.find_element(By.XPATH,\"//p[@class='f4 my-3']\")\n",
    "        Description.append(desc.text)\n",
    "    except NoSuchElementException:\n",
    "        Description.append('-')\n",
    "        \n",
    "        \n",
    "# scraping Contributors Count data\n",
    "    try:\n",
    "        contributor = driver.find_element(By.XPATH,\"//ul[@class='list-style-none ']\")\n",
    "        Contributors.append(contributor.text.replace('Contributors',''))\n",
    "    except NoSuchElementException:\n",
    "        Contributors.append('-')\n",
    "    \n",
    "    \n",
    "# scraping Languages used data\n",
    "    try:\n",
    "        for i in driver.find_elements(By.XPATH,\"//a[@class='d-inline-flex flex-items-center flex-nowrap Link--secondary no-underline text-small mr-3']/span[1]\"):\n",
    "            lang.append(i.text)\n",
    "        Language.append(lang)\n",
    "    except NoSuchElementException:\n",
    "        Language.append('-')\n",
    "       \n",
    "        \n",
    "# Data Framing\n",
    "Github = pd.DataFrame({})\n",
    "Github['Repository URL'] = URLs\n",
    "Github['Repository Title'] = repository_title\n",
    "Github['Repository Description'] = Description\n",
    "Github['Contributors Count'] = Contributors\n",
    "Github['Language Used'] = Language\n",
    "Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e86305",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. \n",
    "Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f24c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https:/www.billboard.com/\")\n",
    "driver.get(url)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9dce30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on charts option button\n",
    "charts=driver.find_element(By.XPATH,\"/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e84835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on billboards hot 100 link\n",
    "bllboards_hot_100=driver.find_element(By.XPATH,\"/html/body/div[3]/main/div[2]/div[1]/div[1]/div/div/div[1]/div[1]/div[2]/span/a\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38966e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Calm Down</td>\n",
       "      <td>Rema &amp; Selena Gomez</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flowers</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All My Life</td>\n",
       "      <td>Lil Durk Featuring J. Cole</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Angel, Pt. 1</td>\n",
       "      <td>Kodak Black, NLE Choppa, Jimin, JVKE &amp; Muni Long</td>\n",
       "      <td>-</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Girl In Mine</td>\n",
       "      <td>Parmalee</td>\n",
       "      <td>-</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Moonlight</td>\n",
       "      <td>Kali Uchis</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Classy 101</td>\n",
       "      <td>Feid x Young Miko</td>\n",
       "      <td>-</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Bluffin</td>\n",
       "      <td>Gucci Mane &amp; Lil Baby</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name                                            Artist  \\\n",
       "0     Last Night                                     Morgan Wallen   \n",
       "1       Fast Car                                        Luke Combs   \n",
       "2      Calm Down                               Rema & Selena Gomez   \n",
       "3        Flowers                                       Miley Cyrus   \n",
       "4    All My Life                        Lil Durk Featuring J. Cole   \n",
       "..           ...                                               ...   \n",
       "95  Angel, Pt. 1  Kodak Black, NLE Choppa, Jimin, JVKE & Muni Long   \n",
       "96  Girl In Mine                                          Parmalee   \n",
       "97     Moonlight                                        Kali Uchis   \n",
       "98    Classy 101                                 Feid x Young Miko   \n",
       "99       Bluffin                             Gucci Mane & Lil Baby   \n",
       "\n",
       "   Last Week Rank Peak Rank Weeks on board  \n",
       "0               1         1             21  \n",
       "1               3         2             13  \n",
       "2               4         3             42  \n",
       "3               2         1             23  \n",
       "4               5         2              6  \n",
       "..            ...       ...            ...  \n",
       "95              -        65              2  \n",
       "96              -        97              1  \n",
       "97             90        80             11  \n",
       "98              -        99              1  \n",
       "99              -       100              1  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty lists\n",
    "Song_Name = []\n",
    "Artist_Name =[]\n",
    "Last_week_rank = []\n",
    "Peak_rank = []\n",
    "Weeks_on_board = []\n",
    "\n",
    "\n",
    "# scraping data of song names\n",
    "for i in driver.find_elements(By.XPATH,\"//li[@class='lrv-u-width-100p']/ul/li/h3\"):\n",
    "    Song_Name.append(i.text)\n",
    "    \n",
    "# scraping data of artist names\n",
    "\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//ul[@class='lrv-a-unstyle-list lrv-u-flex lrv-u-height-100p lrv-u-flex-direction-column@mobile-max']/li[1]/span\"):\n",
    "        Artist_Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Artist_Name.append(\"_\")   \n",
    "# scraping data of last week ranks\n",
    "for i in driver.find_elements(By.XPATH,\"//li[@class='lrv-u-width-100p']/ul/li[4]\"):\n",
    "    Last_week_rank.append(i.text)\n",
    "    \n",
    "\n",
    "# scraping data of peak ranks\n",
    "for i in driver.find_elements(By.XPATH,\"//li[@class='lrv-u-width-100p']/ul/li[5]\"):\n",
    "    Peak_rank.append(i.text)       \n",
    "    \n",
    "    \n",
    "# scraping data of weeks on board\n",
    "for i in driver.find_elements(By.XPATH,\"//li[@class='lrv-u-width-100p']/ul/li[6]\"):\n",
    "    Weeks_on_board.append(i.text)\n",
    "    \n",
    "    \n",
    "# creating dataframe for scraped data\n",
    "billboard = pd.DataFrame({})\n",
    "billboard['Name'] = Song_Name\n",
    "billboard['Artist'] = Artist_Name\n",
    "billboard['Last Week Rank'] = Last_week_rank\n",
    "billboard['Peak Rank'] = Peak_rank\n",
    "billboard['Weeks on board'] = Weeks_on_board\n",
    "billboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3d9a2",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest sellingnovels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e3ff3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row with invalid data: [<td colspan=\"6\">\n",
      "<div class=\"footer\">\n",
      "<div class=\"notes\">\n",
      "<p>SOURCE: <a href=\"//www.nielsenbookscan.co.uk/controller.php?page=48\">NIELSEN BOOK SCAN</a></p>\n",
      "</div>\n",
      "</div> </td>]\n",
      "                                            Book Name         Author Name  \\\n",
      "0                                  Da Vinci Code,The          Brown, Dan    \n",
      "1               Harry Potter and the Deathly Hallows       Rowling, J.K.    \n",
      "2           Harry Potter and the Philosopher's Stone       Rowling, J.K.    \n",
      "3          Harry Potter and the Order of the Phoenix       Rowling, J.K.    \n",
      "4                               Fifty Shades of Grey        James, E. L.    \n",
      "..                                                ...                 ...   \n",
      "95                                         Ghost,The      Harris, Robert    \n",
      "96                    Happy Days with the Naked Chef       Oliver, Jamie    \n",
      "97             Hunger Games,The:Hunger Games Trilogy    Collins, Suzanne    \n",
      "98   Lost Boy,The:A Foster Child's Search for the ...       Pelzer, Dave    \n",
      "99   Jamie's Ministry of Food:Anyone Can Learn to ...      Oliver, Jamie    \n",
      "\n",
      "    Volumes Sold          Publisher                          Genre  \n",
      "0        5094805        Transworld    Crime, Thriller & Adventure   \n",
      "1        4475152        Bloomsbury             Children's Fiction   \n",
      "2        4200654        Bloomsbury             Children's Fiction   \n",
      "3        4179479        Bloomsbury             Children's Fiction   \n",
      "4        3758936      Random House                Romance & Sagas   \n",
      "..           ...                ...                            ...  \n",
      "95        807311      Random House     General & Literary Fiction   \n",
      "96        794201           Penguin          Food & Drink: General   \n",
      "97        792187   Scholastic Ltd.            Young Adult Fiction   \n",
      "98        791507             Orion             Biography: General   \n",
      "99        791095           Penguin          Food & Drink: General   \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# send a request to the URL\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# check if the request was successful\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to load page\")\n",
    "    exit()\n",
    "\n",
    "# parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find the table containing the required information\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# check if the table was found\n",
    "if table is None:\n",
    "    print(\"Table not found\")\n",
    "    exit()\n",
    "\n",
    "# create empty lists to store the data\n",
    "book_names = []\n",
    "author_names = []\n",
    "volumes_sold = []\n",
    "publishers = []\n",
    "genres = []\n",
    "\n",
    "# iterate over each row of the table and extract the data\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    columns = row.find_all(\"td\")\n",
    "    if len(columns) != 6:\n",
    "        print(f\"Skipping row with invalid data: {columns}\")\n",
    "        continue\n",
    "    book_names.append(columns[1].text)\n",
    "    author_names.append(columns[2].text)\n",
    "    volumes_sold.append(int(columns[3].text.replace(\",\", \"\")))\n",
    "    publishers.append(columns[4].text)\n",
    "    genres.append(columns[5].text)\n",
    "\n",
    "# create a dataframe with the extracted data\n",
    "df = pd.DataFrame({\n",
    "    \"Book Name\": book_names,\n",
    "    \"Author Name\": author_names,\n",
    "    \"Volumes Sold\": volumes_sold,\n",
    "    \"Publisher\": publishers,\n",
    "    \"Genre\": genres\n",
    "})\n",
    "\n",
    "# display the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d331a",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com. \n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "66e93335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Name  Year Span                     Genre  \\\n",
      "0                  Game of Thrones  2011–2019  Action, Adventure, Drama   \n",
      "1                  Stranger Things  2016–2024    Drama, Fantasy, Horror   \n",
      "2                 The Walking Dead  2010–2022   Drama, Horror, Thriller   \n",
      "3                   13 Reasons Why  2017–2020  Drama, Mystery, Thriller   \n",
      "4                          The 100  2014–2020    Drama, Mystery, Sci-Fi   \n",
      "..                             ...        ...                       ...   \n",
      "95                           Reign  2013–2017                     Drama   \n",
      "96  A Series of Unfortunate Events  2017–2019  Adventure, Comedy, Drama   \n",
      "97                  Criminal Minds     2005–      Crime, Drama, Mystery   \n",
      "98                          Scream  2015–2019      Comedy, Crime, Drama   \n",
      "99      The Haunting of Hill House       2018    Drama, Horror, Mystery   \n",
      "\n",
      "    Runtime Ratings      Votes  \n",
      "0    57 min          2,172,754  \n",
      "1    51 min          1,250,719  \n",
      "2    44 min          1,032,024  \n",
      "3    60 min            303,451  \n",
      "4    43 min            262,636  \n",
      "..      ...     ...        ...  \n",
      "95   42 min             51,936  \n",
      "96   50 min             63,974  \n",
      "97   42 min            208,473  \n",
      "98   45 min             43,380  \n",
      "99  572 min            259,991  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# send a request to the URL\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# create empty lists to store the data\n",
    "names = []\n",
    "year_spans = []\n",
    "genres = []\n",
    "runtimes = []\n",
    "ratings = []\n",
    "votes = []\n",
    "\n",
    "# find the container containing the required information\n",
    "container = soup.find(\"div\", {\"class\": \"lister-list\"})\n",
    "\n",
    "# iterate over each TV series in the container and extract the data\n",
    "for tv_series in container.find_all(\"div\", {\"class\": \"lister-item-content\"}):\n",
    "    # extract the name of the TV series\n",
    "    name = tv_series.h3.a.text.strip()\n",
    "    names.append(name)\n",
    "    \n",
    "    # extract the year span of the TV series\n",
    "    year_span = tv_series.find(\"span\", {\"class\": \"lister-item-year\"}).text.strip(\"()\")\n",
    "    year_spans.append(year_span)\n",
    "    \n",
    "    # extract the genre of the TV series\n",
    "    genre = tv_series.find(\"span\", {\"class\": \"genre\"}).text.strip()\n",
    "    genres.append(genre)\n",
    "    \n",
    "    # extract the runtime of the TV series\n",
    "    runtime = tv_series.find(\"span\", {\"class\": \"runtime\"}).text.strip()\n",
    "    runtimes.append(runtime)\n",
    "    \n",
    "    # extract the rating of the TV series\n",
    "    rating = tv_series.find(\"div\", {\"class\": \"ipl-rating-star small\"}).span.text.strip()\n",
    "    ratings.append(rating)\n",
    "    \n",
    "    # extract the number of votes for the TV series\n",
    "    vote = tv_series.find(\"span\", {\"name\": \"nv\"}).text.strip()\n",
    "    votes.append(vote)\n",
    "\n",
    "# create a dataframe with the extracted data\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": names,\n",
    "    \"Year Span\": year_spans,\n",
    "    \"Genre\": genres,\n",
    "    \"Runtime\": runtimes,\n",
    "    \"Ratings\": ratings,\n",
    "    \"Votes\": votes\n",
    "})\n",
    "\n",
    "# display the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7bdb8",
   "metadata": {},
   "source": [
    "8. Details of Datasetsfrom UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7217a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\" https://archive.ics.uci.edu/\")\n",
    "driver.get(url)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25351e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking view all dataset button from the webpage\n",
    "viewall_dataset = driver.find_element(By.XPATH,\"//a[@class='btn-primary btn']\").click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75279b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on expand all button\n",
    "expand_all=driver.find_element(By.XPATH,\"//span[@class='swap-on text-primary-content']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "048f3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching urls for each dataset\n",
    "dataset_url = driver.find_elements(By.XPATH,\"//a[@class='link-hover link text-xl font-semibold']\")\n",
    "\n",
    "urls = []\n",
    "for i in dataset_url:\n",
    "    urls.append(i.get_attribute(\"href\"))        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28ecc337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>No of Instance</th>\n",
       "      <th>No of Attributes</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>Donated on 6/30/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>303</td>\n",
       "      <td>13</td>\n",
       "      <td>Donated on 6/30/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48842</td>\n",
       "      <td>14</td>\n",
       "      <td>Donated on 4/30/1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>13611</td>\n",
       "      <td>17</td>\n",
       "      <td>Donated on 9/13/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>-</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rice (Cammeo and Osmancik)</td>\n",
       "      <td>Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>3810</td>\n",
       "      <td>8</td>\n",
       "      <td>Donated on 10/5/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>178</td>\n",
       "      <td>13</td>\n",
       "      <td>Donated on 6/30/1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Car Evaluation</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>1728</td>\n",
       "      <td>6</td>\n",
       "      <td>Donated on 5/31/1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Breast Cancer Wisconsin (Diagnostic)</td>\n",
       "      <td>Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>569</td>\n",
       "      <td>30</td>\n",
       "      <td>Donated on 10/31/199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mushroom</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>8124</td>\n",
       "      <td>22</td>\n",
       "      <td>Donated on 4/26/1987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Data Name                  Data Type   \\\n",
       "0                                  Iris                        Real   \n",
       "1                         Heart Disease  Categorical, Integer, Real   \n",
       "2                                 Adult        Categorical, Integer   \n",
       "3                      Dry Bean Dataset               Integer, Real   \n",
       "4                              Diabetes        Categorical, Integer   \n",
       "5            Rice (Cammeo and Osmancik)                        Real   \n",
       "6                                  Wine               Integer, Real   \n",
       "7                        Car Evaluation                 Categorical   \n",
       "8  Breast Cancer Wisconsin (Diagnostic)                        Real   \n",
       "9                              Mushroom                 Categorical   \n",
       "\n",
       "            Task              Attribute Type  No of Instance   \\\n",
       "0  Classification                        Real             150   \n",
       "1  Classification  Categorical, Integer, Real             303   \n",
       "2  Classification        Categorical, Integer           48842   \n",
       "3  Classification               Integer, Real           13611   \n",
       "4               -        Categorical, Integer               -   \n",
       "5  Classification                        Real            3810   \n",
       "6  Classification               Integer, Real             178   \n",
       "7  Classification                 Categorical            1728   \n",
       "8  Classification                        Real             569   \n",
       "9  Classification                 Categorical            8124   \n",
       "\n",
       "  No of Attributes                  Year   \n",
       "0                 4  Donated on 6/30/1988  \n",
       "1                13  Donated on 6/30/1988  \n",
       "2                14  Donated on 4/30/1996  \n",
       "3                17  Donated on 9/13/2020  \n",
       "4                20                     -  \n",
       "5                 8  Donated on 10/5/2019  \n",
       "6                13  Donated on 6/30/1991  \n",
       "7                 6  Donated on 5/31/1997  \n",
       "8                30  Donated on 10/31/199  \n",
       "9                22  Donated on 4/26/1987  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty lists\n",
    "Dataset_name = []\n",
    "Data_type = []\n",
    "Task = []\n",
    "Attribute_type = []\n",
    "No_of_instances = []\n",
    "No_of_attributes = []\n",
    "Year = []\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping  Dataset name\n",
    "    try:\n",
    "        dataset_name = driver.find_element(By.XPATH,\"//h1[@class='text-3xl font-semibold text-primary-content']\")\n",
    "        Dataset_name.append(dataset_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Dataset_name.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping data type\n",
    "    try:\n",
    "        data_type = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[4]/p\")\n",
    "        if data_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Data_type.append(data_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping Task\n",
    "    try:\n",
    "        task = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[3]/p\")\n",
    "        if task.text == \"N/A\": raise NoSuchElementException\n",
    "        Task.append(task.text)\n",
    "    except NoSuchElementException:\n",
    "        Task.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping Attribute type\n",
    "    try:\n",
    "        attribute_type = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[4]/p\")\n",
    "        if attribute_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Attribute_type.append(attribute_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Attribute_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping No of Instances\n",
    "    try:\n",
    "        instances = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[5]/p\")\n",
    "        if instances.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_instances.append(instances.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_instances.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping No of Arrtibutes\n",
    "    try:\n",
    "        attribute = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[6]/p\")\n",
    "        if attribute.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_attributes.append(attribute.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_attributes.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping Year\n",
    "    try:\n",
    "        year = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[1]/div[2]/h2\")\n",
    "        if year.text == \"N/A\": raise NoSuchElementException\n",
    "        Year.append(year.text[:20])\n",
    "    except NoSuchElementException:\n",
    "        Year.append('-')\n",
    "    time.sleep(3)\n",
    "    # creating dataframe for scraped data\n",
    "ML = pd.DataFrame({})\n",
    "ML['Data Name'] = Dataset_name \n",
    "ML['Data Type '] = Data_type\n",
    "ML['Task '] = Task \n",
    "ML['Attribute Type '] = Attribute_type \n",
    "ML['No of Instance '] = No_of_instances\n",
    "ML['No of Attributes '] = No_of_attributes \n",
    "ML['Year '] = Year \n",
    "ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa37742",
   "metadata": {},
   "source": [
    "9. Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants\n",
    "You have to find the following details: \n",
    "A) Name\n",
    "B) Designation\n",
    "C)Company \n",
    "D)Skills they hire for \n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and \n",
    "click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a37137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.naukri.com/hr-recruiters-consultants\")\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c6d2784",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64c83a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on view more option\n",
    "view_more=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div[1]/div/section[2]/div[2]/div[1]/div[2]/a/span\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dcb3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on department filter\n",
    "dept=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div[1]/div/section[2]/div[2]/div[1]/div[3]/div[2]/div[1]/input\")\n",
    "dept.send_keys(\"data science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca19d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on datascience checkbox\n",
    "datasc=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div[1]/div/section[2]/div[2]/div[1]/div[3]/div[2]/div[2]/div/div/div/label/p/span[1]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49fa9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on apply button\n",
    "apply=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div[1]/div/section[2]/div[2]/div[1]/div[3]/div[2]/div[3]/div[2]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12c364a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unchecking hr default button\n",
    "hr=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[1]/div[2]/div[2]/label/p/span[1]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeebd308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Company</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cargill</td>\n",
       "      <td>FSQR Analytics and Reporting Analyst</td>\n",
       "      <td>Cargill</td>\n",
       "      <td>Performance\\nReporting\\nData Reporting\\nReport...</td>\n",
       "      <td>Mumbai, New Delhi, Pune, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perfect Placer</td>\n",
       "      <td>Business Analyst - Manufacturing Firm</td>\n",
       "      <td>Perfect Placer</td>\n",
       "      <td>Business Analysis\\nReporting\\nReport preparati...</td>\n",
       "      <td>Kolkata, Mumbai, Visakhapatnam, Hyderabad/Secu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IQVIA</td>\n",
       "      <td>Senior Statistical Programmer</td>\n",
       "      <td>IQVIA</td>\n",
       "      <td>Adam\\nSafety tables\\nstatistical programming\\n...</td>\n",
       "      <td>Hybrid - Kochi/Cochin, Hyderabad/Secunderabad,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infogain</td>\n",
       "      <td>Immediate Opening For Azure Architect (Data En...</td>\n",
       "      <td>Infogain</td>\n",
       "      <td>Azure Data Factory\\nAzure\\nData Bricks\\nSQL\\nP...</td>\n",
       "      <td>Hybrid - Noida, Pune, Bangalore/Bengaluru, Mum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hexaware Technologies</td>\n",
       "      <td>Matillion and Snowflake Developer with Hexawar...</td>\n",
       "      <td>Hexaware Technologies</td>\n",
       "      <td>Matillion\\nSnowflake\\nDevelopment\\nAWS</td>\n",
       "      <td>Hybrid - Pune, Maharashtra, Chennai, Bangalore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Genpact</td>\n",
       "      <td>Urgent Hiring Qlik Sense Developer, Bangalore ...</td>\n",
       "      <td>Genpact</td>\n",
       "      <td>front end\\ndata modeling\\nQlik Sense Developme...</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Bfsi\\nConsulting\\nMachine learning\\nOpen sourc...</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>ACN - Applied Intelligence - Finance - Data Sc...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Data Science\\nApplied Intelligence\\nData proce...</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Gurgaon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>ACN - Applied Intelligence - Finance - Data Sc...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Analytics\\nCoding\\nOpen source\\nPython\\nJenkin...</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Gurgaon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tech Mahindra</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "      <td>Power Bi\\nAzure Functions\\nSQL\\nData analysis\\...</td>\n",
       "      <td>Hybrid - Hyderabad/ Secunderabad, Telangana, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cognizant</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Cognizant</td>\n",
       "      <td>Hive\\ndata modeling\\nHadoop\\nHBase\\nData Engin...</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Bangalo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>Senior\\nSupply chain\\nAnalytical\\nAnalytics\\nS...</td>\n",
       "      <td>Mumbai, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Broadridge</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Broadridge</td>\n",
       "      <td>Performance tuning\\nSQL\\nPerformance\\nBusiness...</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL</td>\n",
       "      <td>GCP\\nData Science\\nDeep Learning\\nData\\nMachin...</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL</td>\n",
       "      <td>Business Systems Analyst</td>\n",
       "      <td>AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL</td>\n",
       "      <td>System\\nCustomer service\\nSubject\\nRecruitment...</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Randstad India</td>\n",
       "      <td>AI ML Engineer For Bangalore Location</td>\n",
       "      <td>Randstad India</td>\n",
       "      <td>AWS\\nMachine Learning\\nPython\\nMl\\nPytorch\\nAl...</td>\n",
       "      <td>Hybrid - Pune, Maharashtra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HCLTech</td>\n",
       "      <td>SQL Developer - Tibco Data Virtualization</td>\n",
       "      <td>HCLTech</td>\n",
       "      <td>Data Transformation\\nIndex Optimization\\nSQL Q...</td>\n",
       "      <td>Noida, Hyderabad/Secunderabad, Pune, Chennai, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Wipro</td>\n",
       "      <td>Data Analytics Architect-IBM MDM</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>ibm mdm\\noracle\\nsoap web services\\ndbms\\nRAD\\...</td>\n",
       "      <td>Hyderabad/ Secunderabad, Telangana, Pune, Maha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Siemens</td>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Siemens</td>\n",
       "      <td>Data Engineering\\nTechnology\\nNoSQL\\nData mode...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pricewaterhouse Coopers Service Delivery Cente...</td>\n",
       "      <td>Data Analyst_SSIS_SSRS_SQL_Senior Associate_BL...</td>\n",
       "      <td>Pricewaterhouse Coopers Service Delivery Cente...</td>\n",
       "      <td>ssrs\\njson\\nbusiness analysis\\nssis\\nsql\\nPyth...</td>\n",
       "      <td>Hybrid - Kolkata, Hyderabad/Secunderabad, Bang...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0                                             Cargill   \n",
       "1                                      Perfect Placer   \n",
       "2                                               IQVIA   \n",
       "3                                            Infogain   \n",
       "4                               Hexaware Technologies   \n",
       "5                                             Genpact   \n",
       "6                                           Accenture   \n",
       "7                                           Accenture   \n",
       "8                                           Accenture   \n",
       "9                                       Tech Mahindra   \n",
       "10                                          Cognizant   \n",
       "11                                  Fractal Analytics   \n",
       "12                                         Broadridge   \n",
       "13            AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL   \n",
       "14            AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL   \n",
       "15                                     Randstad India   \n",
       "16                                            HCLTech   \n",
       "17                                              Wipro   \n",
       "18                                            Siemens   \n",
       "19  Pricewaterhouse Coopers Service Delivery Cente...   \n",
       "\n",
       "                                          Designation  \\\n",
       "0                FSQR Analytics and Reporting Analyst   \n",
       "1               Business Analyst - Manufacturing Firm   \n",
       "2                       Senior Statistical Programmer   \n",
       "3   Immediate Opening For Azure Architect (Data En...   \n",
       "4   Matillion and Snowflake Developer with Hexawar...   \n",
       "5   Urgent Hiring Qlik Sense Developer, Bangalore ...   \n",
       "6                    Analystics & Modeling Specialist   \n",
       "7   ACN - Applied Intelligence - Finance - Data Sc...   \n",
       "8   ACN - Applied Intelligence - Finance - Data Sc...   \n",
       "9                                        Data Analyst   \n",
       "10                                      Data Engineer   \n",
       "11                              Senior Data Scientist   \n",
       "12                                   Business Analyst   \n",
       "13                              Senior Data Scientist   \n",
       "14                           Business Systems Analyst   \n",
       "15              AI ML Engineer For Bangalore Location   \n",
       "16          SQL Developer - Tibco Data Virtualization   \n",
       "17                   Data Analytics Architect-IBM MDM   \n",
       "18                                 Lead Data Engineer   \n",
       "19  Data Analyst_SSIS_SSRS_SQL_Senior Associate_BL...   \n",
       "\n",
       "                                              Company  \\\n",
       "0                                             Cargill   \n",
       "1                                      Perfect Placer   \n",
       "2                                               IQVIA   \n",
       "3                                            Infogain   \n",
       "4                               Hexaware Technologies   \n",
       "5                                             Genpact   \n",
       "6                                           Accenture   \n",
       "7                                           Accenture   \n",
       "8                                           Accenture   \n",
       "9                                       Tech Mahindra   \n",
       "10                                          Cognizant   \n",
       "11                                  Fractal Analytics   \n",
       "12                                         Broadridge   \n",
       "13            AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL   \n",
       "14            AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL   \n",
       "15                                     Randstad India   \n",
       "16                                            HCLTech   \n",
       "17                                              Wipro   \n",
       "18                                            Siemens   \n",
       "19  Pricewaterhouse Coopers Service Delivery Cente...   \n",
       "\n",
       "                                               Skills  \\\n",
       "0   Performance\\nReporting\\nData Reporting\\nReport...   \n",
       "1   Business Analysis\\nReporting\\nReport preparati...   \n",
       "2   Adam\\nSafety tables\\nstatistical programming\\n...   \n",
       "3   Azure Data Factory\\nAzure\\nData Bricks\\nSQL\\nP...   \n",
       "4              Matillion\\nSnowflake\\nDevelopment\\nAWS   \n",
       "5   front end\\ndata modeling\\nQlik Sense Developme...   \n",
       "6   Bfsi\\nConsulting\\nMachine learning\\nOpen sourc...   \n",
       "7   Data Science\\nApplied Intelligence\\nData proce...   \n",
       "8   Analytics\\nCoding\\nOpen source\\nPython\\nJenkin...   \n",
       "9   Power Bi\\nAzure Functions\\nSQL\\nData analysis\\...   \n",
       "10  Hive\\ndata modeling\\nHadoop\\nHBase\\nData Engin...   \n",
       "11  Senior\\nSupply chain\\nAnalytical\\nAnalytics\\nS...   \n",
       "12  Performance tuning\\nSQL\\nPerformance\\nBusiness...   \n",
       "13  GCP\\nData Science\\nDeep Learning\\nData\\nMachin...   \n",
       "14  System\\nCustomer service\\nSubject\\nRecruitment...   \n",
       "15  AWS\\nMachine Learning\\nPython\\nMl\\nPytorch\\nAl...   \n",
       "16  Data Transformation\\nIndex Optimization\\nSQL Q...   \n",
       "17  ibm mdm\\noracle\\nsoap web services\\ndbms\\nRAD\\...   \n",
       "18  Data Engineering\\nTechnology\\nNoSQL\\nData mode...   \n",
       "19  ssrs\\njson\\nbusiness analysis\\nssis\\nsql\\nPyth...   \n",
       "\n",
       "                                             Location  \n",
       "0        Mumbai, New Delhi, Pune, Bangalore/Bengaluru  \n",
       "1   Kolkata, Mumbai, Visakhapatnam, Hyderabad/Secu...  \n",
       "2   Hybrid - Kochi/Cochin, Hyderabad/Secunderabad,...  \n",
       "3   Hybrid - Noida, Pune, Bangalore/Bengaluru, Mum...  \n",
       "4   Hybrid - Pune, Maharashtra, Chennai, Bangalore...  \n",
       "5   Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...  \n",
       "6   Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...  \n",
       "7   Hyderabad/Secunderabad, Pune, Chennai, Gurgaon...  \n",
       "8   Hyderabad/Secunderabad, Pune, Chennai, Gurgaon...  \n",
       "9   Hybrid - Hyderabad/ Secunderabad, Telangana, P...  \n",
       "10  Hyderabad/Secunderabad, Pune, Chennai, Bangalo...  \n",
       "11                        Mumbai, Bangalore/Bengaluru  \n",
       "12        Hyderabad/Secunderabad, Bangalore/Bengaluru  \n",
       "13  Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...  \n",
       "14  Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...  \n",
       "15                         Hybrid - Pune, Maharashtra  \n",
       "16  Noida, Hyderabad/Secunderabad, Pune, Chennai, ...  \n",
       "17  Hyderabad/ Secunderabad, Telangana, Pune, Maha...  \n",
       "18                                Bangalore/Bengaluru  \n",
       "19  Hybrid - Kolkata, Hyderabad/Secunderabad, Bang...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty lists\n",
    "Name = []\n",
    "Designation = []\n",
    "Company = []\n",
    "Skills = []\n",
    "Location = []\n",
    "\n",
    "# scraping data of Names\n",
    "for i in driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\"):\n",
    "    Name.append(i.text)\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Designation\n",
    "for i in driver.find_elements(By.XPATH,\"//a[@class='title ellipsis']\"):\n",
    "    Designation.append(i.text)\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Company Name\n",
    "for i in driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\"):\n",
    "    Company.append(i.text)\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Skills\n",
    "for i in driver.find_elements(By.XPATH,\"//ul[@class='tags has-description']\"):\n",
    "    try:\n",
    "        if i.text == \"Not Specified\": raise NoSuchElementException\n",
    "        Skills.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Skills.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Location\n",
    "for i in driver.find_elements(By.XPATH,\"//li[@class='fleft br2 placeHolderLi location']\"):\n",
    "    try:\n",
    "        if i.text == \"Not Specified\": raise NoSuchElementException\n",
    "        Location.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Location.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# creating dataframe for scraped data\n",
    "Naukri = pd.DataFrame({})\n",
    "Naukri['Name'] = Name[:49]\n",
    "Naukri['Designation'] = Designation[:49]\n",
    "Naukri['Company'] = Company[:49]\n",
    "Naukri['Skills'] = Skills[:49]\n",
    "Naukri['Location'] = Location[:49]\n",
    "Naukri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88b4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
